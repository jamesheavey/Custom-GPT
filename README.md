# Custom-GPT
In this codebase I build a custom Generatively Pretrained Transformer (GPT), following the paper "Attention is All You Need"

In this code base I create a simple transformer using character encoding (rather than sub-word token encoding) following a lecture given by Andrej Karpathy. You can find the lecture [here](https://www.youtube.com/watch?v=kCc8FmEb1nY).
